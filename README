# Fraud Detection Service – README

## Table of Contents
1. [Architecture & Concepts](#architecture--concepts)  
2. [Repository Layout](#repository-layout)  
3. [Prerequisites](#prerequisites)  
4. [Quickstart](#quickstart)  
   - [Option A — Local (Conda)](#option-a--local-conda)  
   - [Option B — Local (Docker Compose)](#option-b--local-docker-compose)  
   - [Option C — Production Deploy (Remote Server)](#option-c--production-deploy-remote-server)  
5. [Environment Variables](#environment-variables)  
6. [API Overview](#api-overview)  
7. [Machine Learning Details](#machine-learning-details)  
8. [Using Postman Safely (Idempotency)](#using-postman-safely-idempotency)  
9. [Operations: Retraining & Auto‑Deployment](#operations-retraining--auto-deployment)  
10. [Troubleshooting](#troubleshooting)

---

## Architecture & Concepts

### System Flow
At a high level, transactions are ingested, features are extracted, a deployed model assigns a fraud risk score, and transactions above a threshold are flagged for human review. Confirmed outcomes feed back into training data, and better models are periodically deployed. fileciteturn0file3

### Risk Scoring & Thresholding
Risk analysis considers factors including amount, time, frequency, pattern, and velocity with defined weights; high combined risk can trigger automatic flags, temporary blocking, and escalation for review. The system also keeps recent history for pattern detection and auditability. fileciteturn0file1

### Why ML (Gradient Boosting) vs Rules
Rule‑only systems are hard to maintain, don’t adapt quickly to new patterns, and struggle with complex interactions. Gradient Boosting addresses these gaps by handling imbalanced data, surfacing feature importance, and capturing nonlinear patterns in structured transaction data. fileciteturn0file4  
For an approachable explanation of Gradient Boosting as an ensemble that iteratively corrects previous errors, see the included primer. fileciteturn0file2

---

## Repository Layout
```
.
├─ deploy.sh                # One‑command remote deployment via rsync + SSH
├─ local.yml                # Docker Compose for local development (services defined here)
├─ production.yml           # Docker Compose for production on the remote host
├─ explanation.md           # System breakdown: risk factors, scoring, thresholds, responses
├─ gradient-boosting.md     # Plain‑English intro to Gradient Boosting
├─ how_it_works.md          # End‑to‑end pipeline and deployment modes (manual/auto)
├─ ml-pipeline.md           # (If present) additional pipeline notes
├─ postman-prescript.js     # Pre‑request script to auto‑add Idempotency-Key header
└─ (backend/, migration/, etc.)    # Your app and ML code (structure)
```


> **Note:** `local.yml` and `production.yml` are Compose descriptors you’ll run with `docker compose -f ...`. Their exact services/ports depend on your project definition.

---

## Prerequisites

- **Git** (to fetch sources)
- **Conda** (Miniconda/Anaconda) for a reproducible Python environment
- **Docker & Docker Compose** (for containerized local/production runs)
- **SSH access** to your remote host (for production deployment with `deploy.sh`)

---

## Quickstart

### Option A — Local (Conda)

1. **Install/Update Conda (recommended: Miniconda).**  
2. **Create and activate an environment.** Replace the Python version as needed:
   ```
   conda create -n fraudml python=3.10 -y
   conda activate fraudml
   ```
3. **Install dependencies.** If you have a `requirements.txt` or `pyproject.toml`:
   ```
   # If using requirements.txt
   pip install -r requirements.txt

   # Or, if using Poetry
   # pip install poetry && poetry install
   ```
4. **Configure environment variables.** See [Environment Variables](#environment-variables).  
5. **Run your app or notebooks.**  
   - If you provide a CLI or app server entrypoint, run it here (e.g., `python -m your_module`).
   - Use this Conda env for model experimentation and scripts.
6. **Deactivate when finished:**
   ```
   conda deactivate
   ```

### Option B — Local (Docker Compose)

Build and start services defined in `local.yml`:
```
docker compose -f local.yml up --build
```
Stop them (and remove containers) when done:
```
docker compose -f local.yml down
```

### Option C — Production Deploy (Remote Server)

1. **Ensure remote host is ready** (Docker installed, SSH key access).  
2. **Set the droplet/server IP address in your shell** (required by the script):
   ```
   export DIGITAL_OCEAN_IP_ADDRESS=YOUR.SERVER.IP
   ```
3. **Run the deploy script from the repo root:**
   ```
   bash deploy.sh
   ```
   What it does, in brief:
   - Archives the **`main`** branch into a tarball.  
   - Uploads the archive via `rsync` to `/tmp/project.tar` on the server.  
   - SSHes in, extracts to a temporary directory, brings down existing containers via the production Compose file, prunes unused Docker artifacts, and brings the new stack up with `--build -d --remove-orphans`. 


---

## Environment Variables

Create a `.env` (or export in your shell) with values your services need. For deployment, you **must** set:
- `DIGITAL_OCEAN_IP_ADDRESS` — The remote server IP read by `deploy.sh`. 

Service‑specific variables (DB URLs, API keys, etc.) depend on your app code and Compose files.

---

## API Overview

The service supports two model deployment modes:

- **Manual deployment:** Select and deploy a model by ID via `/api/v1/ml/deploy`.  
- **Auto‑deployment:** Automatically select the best performing model above threshold with `/api/v1/ml/auto-deploy`. 

Your application will also have endpoints to score transactions and to review flagged cases, according to your implementation.

---

## Machine Learning Details

- **Rule engine vs ML:** motivates moving beyond rules to a Gradient Boosting model that adapts to new patterns and handles complex, nonlinear interactions. fileciteturn0file4
- **Gradient Boosting (plain‑English):** an ensemble of learners that iteratively correct one another’s errors to improve predictions. fileciteturn0file2
- **Risk factors & weights:** amount (30%), frequency (20%), pattern (20%), velocity (20%), time (10%); combined risk above thresholds triggers blocking, review, and logging. fileciteturn0file1
- **End‑to‑end pipeline:** from transaction ingestion and feature extraction through scoring, human review, feedback, retraining, and (auto)deployment. fileciteturn0file3

---



